{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOFhyYsa59Mp9iHfnRzI38Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZHAOTIEZHU2333/COMP3702-A1-Code-2025-main/blob/main/Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6YKRgsxHK1O",
        "outputId": "f2557484-a685-4fdd-b596-f230cce8af87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DNCSg8rbCPYw",
        "outputId": "d9d753bf-bf78-4192-998e-87200bde1215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] device=cuda\n",
            "[DATA] subjects: train=10196 val=1132\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1755100074.py:304: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
            "/tmp/ipython-input-1755100074.py:311: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type==\"cuda\")):\n",
            "/tmp/ipython-input-1755100074.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAE] epoch 1/3 | loss=0.4097\n",
            "[VAE] epoch 2/3 | loss=0.3948\n",
            "[VAE] epoch 3/3 | loss=0.3935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1755100074.py:329: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
            "/tmp/ipython-input-1755100074.py:338: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device.type==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1755100074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1755100074.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mtransplant_encoder_from_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     unet = train_unet(unet, (tr_loader, va_loader), epochs=args.unet_epochs,\n\u001b[0m\u001b[1;32m    402\u001b[0m                       device=device, n_classes=args.classes, ignore_index=args.ignore_index, lr=1e-3)\n\u001b[1;32m    403\u001b[0m     \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1755100074.py\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m(unet, loaders, epochs, device, n_classes, ignore_index, lr)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mall_dsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mhas_complex\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "import os, math, glob, argparse, random, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Sequence, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "    _HAVE_PIL = True\n",
        "except Exception:\n",
        "    _HAVE_PIL = False\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def pick_device():\n",
        "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def zscore(x: np.ndarray, eps=1e-6):\n",
        "    m, s = x.mean(), x.std()\n",
        "    return (x - m) / (s + eps)\n",
        "\n",
        "def minmax01(x: np.ndarray, eps=1e-6):\n",
        "    mi, ma = x.min(), x.max()\n",
        "    return (x - mi) / (ma - mi + eps)\n",
        "\n",
        "class OASISSliceDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 items: Sequence[Tuple[Path, Optional[Path]]],\n",
        "                 plane: str = \"axial\",\n",
        "                 slice_step: int = 1,\n",
        "                 classes: int = 4,\n",
        "                 for_vae: bool = False,\n",
        "                 ignore_label: Optional[int] = None):\n",
        "        self.items = list(items)\n",
        "        self.plane = plane\n",
        "        self.slice_step = slice_step\n",
        "        self.classes = classes\n",
        "        self.for_vae = for_vae\n",
        "        self.ignore_label = ignore_label\n",
        "        self.slices: List[Tuple[Path, Optional[Path], int]] = []\n",
        "        self._index_slices()\n",
        "\n",
        "    def _index_slices(self):\n",
        "        for (ip, mp) in self.items:\n",
        "            img = self._load_volume(ip)\n",
        "            D, H, W = img.shape\n",
        "            if mp is not None:\n",
        "                msk = self._load_volume(mp).astype(np.int64)\n",
        "                assert msk.shape == img.shape, f\"Mask shape mismatch for {ip}\"\n",
        "            else:\n",
        "                msk = None\n",
        "            depth = D\n",
        "            for s in range(0, depth, self.slice_step):\n",
        "                self.slices.append((ip, mp, s))\n",
        "\n",
        "    def _load_volume(self, p: Path) -> np.ndarray:\n",
        "        # Corrected to handle single image files which might not be 3D\n",
        "        img = Image.open(p).convert(\"L\")\n",
        "        arr = np.array(img, dtype=np.float32)\n",
        "        if arr.ndim == 2:\n",
        "            arr = arr[None, ...] # Add channel dimension for 2D images\n",
        "        return arr\n",
        "\n",
        "    def _extract(self, vol: np.ndarray, s: int) -> np.ndarray:\n",
        "        # This method assumes a 3D volume, need to handle 2D case\n",
        "        if vol.ndim == 3:\n",
        "             return vol[s, :, :]\n",
        "        elif vol.ndim == 2: # Handle case where the input is already a 2D slice\n",
        "             return vol\n",
        "        else:\n",
        "             raise ValueError(f\"Unexpected volume dimension: {vol.ndim}\")\n",
        "\n",
        "    def __len__(self): return len(self.slices)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        ip, mp, s = self.slices[idx]\n",
        "        img3d = self._load_volume(ip).astype(np.float32)\n",
        "        # Handle cases where _load_volume returns a 2D image directly\n",
        "        if img3d.ndim == 2:\n",
        "            img2d = img3d\n",
        "        else:\n",
        "            img2d = self._extract(img3d, s)\n",
        "\n",
        "        img2d = zscore(img2d)\n",
        "        img2d = np.clip(img2d, -5, 5)\n",
        "        img2d = minmax01(img2d)\n",
        "        img = torch.from_numpy(img2d[None, ...]).float()\n",
        "\n",
        "        if self.for_vae or (mp is None):\n",
        "            # Return a dummy mask if for VAE or no mask is provided\n",
        "            # Assuming a consistent image size for dummy mask\n",
        "            dummy_mask = torch.zeros(img.shape[2:], dtype=torch.long)\n",
        "            return img, dummy_mask # Returning a dummy mask instead of a scalar 0\n",
        "\n",
        "        msk3d = self._load_volume(mp).astype(np.int64)\n",
        "        # Handle cases where _load_volume returns a 2D image directly\n",
        "        if msk3d.ndim == 2:\n",
        "            msk2d = msk3d\n",
        "        else:\n",
        "            msk2d = self._extract(msk3d, s)\n",
        "\n",
        "        uniq = sorted(int(v) for v in np.unique(msk2d))\n",
        "        bg = 0 if 0 in uniq else uniq[0]\n",
        "        non_bg = [v for v in uniq if v != bg]\n",
        "        keep = [bg] + non_bg[:max(0, self.classes - 1)]\n",
        "        lut = {v: i for i, v in enumerate(keep)}\n",
        "        msk2d = np.vectorize(lambda v: lut.get(int(v), 0))(msk2d).astype(np.int64)\n",
        "\n",
        "        msk = torch.from_numpy(msk2d).long()\n",
        "        return img, msk\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim=64):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 2, 1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.to_mu = nn.Conv2d(128, z_dim, 1)\n",
        "        self.to_logv = nn.Conv2d(128, z_dim, 1)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 128, 2, 2), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, 2, 2), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, 2, 2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 1, 3, 1, 1), nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def reparam(self, mu, logv):\n",
        "        std = (0.5 * logv).exp()\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.enc(x)\n",
        "        mu = self.to_mu(h)\n",
        "        logv = self.to_logv(h)\n",
        "        z = self.reparam(mu, logv)\n",
        "        xrec = self.dec(z)\n",
        "        return xrec, mu, logv\n",
        "\n",
        "def kld_loss(mu, logv):\n",
        "    return 0.5 * torch.mean(torch.sum(torch.exp(logv) + mu**2 - 1.0 - logv, dim=[1,2,3]))\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, 1, 1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNet2D(nn.Module):\n",
        "    def __init__(self, in_ch=1, n_classes=4, base=32):\n",
        "        super().__init__()\n",
        "        self.d1 = DoubleConv(in_ch, base)\n",
        "        self.p1 = nn.MaxPool2d(2)\n",
        "        self.d2 = DoubleConv(base, base*2)\n",
        "        self.p2 = nn.MaxPool2d(2)\n",
        "        self.d3 = DoubleConv(base*2, base*4)\n",
        "        self.p3 = nn.MaxPool2d(2)\n",
        "        self.bottleneck = DoubleConv(base*4, base*8)\n",
        "        self.u3 = nn.ConvTranspose2d(base*8, base*4, 2, 2)\n",
        "        self.u2 = nn.ConvTranspose2d(base*4, base*2, 2, 2)\n",
        "        self.u1 = nn.ConvTranspose2d(base*2, base,   2, 2)\n",
        "        self.c3 = DoubleConv(base*8, base*4)\n",
        "        self.c2 = DoubleConv(base*4, base*2)\n",
        "        self.c1 = DoubleConv(base*2, base)\n",
        "        self.head = nn.Conv2d(base, n_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.d1(x)\n",
        "        d2 = self.d2(self.p1(d1))\n",
        "        d3 = self.d3(self.p2(d2))\n",
        "        bn = self.bottleneck(self.p3(d3))\n",
        "        x  = self.u3(bn)\n",
        "        x  = self.c3(torch.cat([x, d3], dim=1))\n",
        "        x  = self.u2(x)\n",
        "        x  = self.c2(torch.cat([x, d2], dim=1))\n",
        "        x  = self.u1(x)\n",
        "        x  = self.c1(torch.cat([x, d1], dim=1))\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "def transplant_encoder_from_vae(unet: UNet2D, vae: VAE):\n",
        "    vae_layers = [m for m in vae.enc.modules() if isinstance(m, nn.Conv2d)]\n",
        "    unet_layers = []\n",
        "    for block in [unet.d1, unet.d2, unet.d3]:\n",
        "        for m in block.modules():\n",
        "            if isinstance(m, nn.Conv2d): unet_layers.append(m)\n",
        "    k = min(len(vae_layers), len(unet_layers))\n",
        "    with torch.no_grad():\n",
        "        for i in range(k):\n",
        "            if vae_layers[i].weight.shape == unet_layers[i].weight.shape:\n",
        "                unet_layers[i].weight.copy_(vae_layers[i].weight)\n",
        "                if (vae_layers[i].bias is not None) and (unet_layers[i].bias is not None):\n",
        "                    unet_layers[i].bias.copy_(vae_layers[i].bias)\n",
        "\n",
        "class SoftDiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0, ignore_index: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits, targets_onehot):\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        dims = (0,2,3)\n",
        "        intersection = torch.sum(probs * targets_onehot, dim=dims)\n",
        "        cardinality = torch.sum(probs + targets_onehot, dim=dims)\n",
        "        dice = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
        "        return 1. - dice.mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def dice_per_class(logits, targets, num_classes, ignore_index: Optional[int] = None):\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    pred = probs.argmax(dim=1)\n",
        "    dices = []\n",
        "    for c in range(num_classes):\n",
        "        if (ignore_index is not None) and (c == ignore_index):\n",
        "            dices.append(float(\"nan\"))\n",
        "            continue\n",
        "        pc = (pred == c).float()\n",
        "        tc = (targets == c).float()\n",
        "        inter = (pc*tc).sum().item()\n",
        "        denom = pc.sum().item() + tc.sum().item()\n",
        "        d = (2*inter + 1.0) / (denom + 1.0)\n",
        "        dices.append(d)\n",
        "    return dices\n",
        "\n",
        "import re\n",
        "\n",
        "def discover_pairs_keras(root: Path):\n",
        "    img_dirs = [\n",
        "        root / \"keras_png_slices_train\",\n",
        "        root / \"keras_png_slices_validate\",\n",
        "        root / \"keras_png_slices_test\",\n",
        "    ]\n",
        "    seg_dirs = [\n",
        "        root / \"keras_png_slices_seg_train\",\n",
        "        root / \"keras_png_slices_seg_validate\",\n",
        "        root / \"keras_png_slices_seg_test\",\n",
        "    ]\n",
        "\n",
        "    # Check if root directory exists\n",
        "    if not root.exists():\n",
        "        print(f\"[ERROR] Data root directory not found: {root}\")\n",
        "        return []\n",
        "\n",
        "    # Check if expected subdirectories exist\n",
        "    for d in img_dirs + seg_dirs:\n",
        "        if not d.exists():\n",
        "            print(f\"[ERROR] Expected data subdirectory not found: {d}\")\n",
        "            print(\"Please ensure your data is organized correctly within the data-root directory.\")\n",
        "            return []\n",
        "\n",
        "    pairs = []\n",
        "    for idr, sdr in zip(img_dirs, seg_dirs):\n",
        "        img_map = {}\n",
        "        for ip in idr.glob(\"*.png\"):\n",
        "            # Extract the common part from the image file name\n",
        "            m = re.search(r'^case_(.+)\\.nii\\.png$', ip.name)\n",
        "            if m:\n",
        "                key = m.group(1)\n",
        "                img_map[key] = ip\n",
        "\n",
        "        seg_map = {}\n",
        "        for sp in sdr.glob(\"*.png\"):\n",
        "            # Extract the common part from the segmentation file name\n",
        "            m = re.search(r'^seg_(.+)\\.nii\\.png$', sp.name)\n",
        "            if m:\n",
        "                key = m.group(1)\n",
        "                seg_map[key] = sp\n",
        "\n",
        "        # Find common keys (base names) and create pairs\n",
        "        common_keys = img_map.keys() & seg_map.keys()\n",
        "        for k in sorted(common_keys):\n",
        "            pairs.append((img_map[k], seg_map[k]))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def split_train_val(items, val_ratio=0.1, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    idx = np.arange(len(items)); rng.shuffle(idx)\n",
        "    n_val = max(1, int(len(items)*val_ratio))\n",
        "    val_idx, tr_idx = idx[:n_val], idx[n_val:]\n",
        "    tr = [items[i] for i in tr_idx]\n",
        "    va = [items[i] for i in val_idx]\n",
        "    return tr, va\n",
        "\n",
        "def train_vae(vae, loader, epochs, device, lr=1e-3):\n",
        "    vae = vae.to(device)\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "    for ep in range(1, epochs+1):\n",
        "        vae.train()\n",
        "        total = 0.0\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
        "                xrec, mu, logv = vae(x)\n",
        "            with torch.amp.autocast(device_type='cpu', enabled=False):\n",
        "                rec = F.binary_cross_entropy(xrec.float(), x.float(), reduction=\"mean\")\n",
        "                kld = kld_loss(mu.float(), logv.float())\n",
        "                loss = rec + 1e-3 * kld\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "            total += loss.item()*x.size(0)\n",
        "        print(f\"[VAE] epoch {ep}/{epochs} | loss={total/len(loader.dataset):.4f}\")\n",
        "    return vae\n",
        "\n",
        "def train_unet(unet, loaders, epochs, device, n_classes, ignore_index=None, lr=1e-3):\n",
        "    train_loader, val_loader = loaders\n",
        "    unet = unet.to(device)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    dice = SoftDiceLoss(ignore_index=ignore_index)\n",
        "    opt = torch.optim.Adam(unet.parameters(), lr=lr)\n",
        "    scaler = torch.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "    best = {\"epoch\":0, \"mean_dsc\":0.0}\n",
        "    for ep in range(1, epochs+1):\n",
        "        unet.train()\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            y1h = F.one_hot(y, n_classes).permute(0,3,1,2).float()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type==\"cuda\")):\n",
        "                logits = unet(x)\n",
        "                loss = ce(logits, y) + dice(logits, y1h)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "        unet.eval()\n",
        "        all_dsc = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x = x.to(device); y = y.to(device)\n",
        "                logits = unet(x)\n",
        "                dscs = dice_per_class(logits, y, n_classes, ignore_index)\n",
        "                all_dsc.append(np.array(dscs))\n",
        "        dsc_mean = float(np.nanmean(np.vstack(all_dsc), axis=0).mean())\n",
        "        print(f\"[UNet] epoch {ep}/{epochs} | mean DSC={dsc_mean:.4f}\")\n",
        "        if dsc_mean > best[\"mean_dsc\"]:\n",
        "            best = {\"epoch\": ep, \"mean_dsc\": dsc_mean}\n",
        "    print(f\"[UNet] best mean DSC={best['mean_dsc']:.4f} @ epoch {best['epoch']}\")\n",
        "    return unet\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data-root\", type=str, default=\"/content/drive/MyDrive/Colab Notebooks/keras_png_slices_data\")\n",
        "    ap.add_argument(\"--plane\", type=str, default=\"axial\", choices=[\"axial\",\"sagittal\",\"coronal\"])\n",
        "    ap.add_argument(\"--slice-step\", type=int, default=1)\n",
        "    ap.add_argument(\"--classes\", type=int, default=4)\n",
        "    ap.add_argument(\"--ignore-index\", type=int, default=None)\n",
        "    ap.add_argument(\"--batch\", type=int, default=32)\n",
        "    ap.add_argument(\"--workers\", type=int, default=2)\n",
        "    ap.add_argument(\"--vae-epochs\", type=int, default=3)\n",
        "    ap.add_argument(\"--unet-epochs\", type=int, default=10)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--out\", type=str, default=\"runs_part4_colab\")\n",
        "    args, _ = ap.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    seed_everything(args.seed)\n",
        "    device = pick_device()\n",
        "    print(f\"[INFO] device={device}\")\n",
        "    root = Path(args.data_root)\n",
        "    pairs = discover_pairs_keras(root)\n",
        "    assert len(pairs) > 0, f\"No image/mask png pairs found under {root}\"\n",
        "    train_items, val_items = split_train_val(pairs, val_ratio=0.1, seed=args.seed)\n",
        "    print(f\"[DATA] subjects: train={len(train_items)} val={len(val_items)}\")\n",
        "    vae_train = OASISSliceDataset(train_items, plane=args.plane, slice_step=args.slice_step,\n",
        "                                  classes=args.classes, for_vae=True)\n",
        "    seg_train = OASISSliceDataset(train_items, plane=args.plane, slice_step=args.slice_step,\n",
        "                                  classes=args.classes, for_vae=False, ignore_label=args.ignore_index)\n",
        "    seg_val   = OASISSliceDataset(val_items,   plane=args.plane, slice_step=args.slice_step,\n",
        "                                  classes=args.classes, for_vae=False, ignore_label=args.ignore_index)\n",
        "    vae_loader = DataLoader(vae_train, batch_size=args.batch, shuffle=True,\n",
        "                            num_workers=args.workers, pin_memory=(device.type==\"cuda\"))\n",
        "    tr_loader  = DataLoader(seg_train, batch_size=args.batch, shuffle=True,\n",
        "                            num_workers=args.workers, pin_memory=(device.type==\"cuda\"))\n",
        "    va_loader  = DataLoader(seg_val,   batch_size=args.batch, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=(device.type==\"cuda\"))\n",
        "    vae = VAE(z_dim=64)\n",
        "    vae = train_vae(vae, vae_loader, epochs=args.vae_epochs, device=device, lr=1e-3)\n",
        "    unet = UNet2D(in_ch=1, n_classes=args.classes, base=32)\n",
        "    transplant_encoder_from_vae(unet, vae)\n",
        "    unet = train_unet(unet, (tr_loader, va_loader), epochs=args.unet_epochs,\n",
        "                      device=device, n_classes=args.classes, ignore_index=args.ignore_index, lr=1e-3)\n",
        "    unet.eval()\n",
        "    all_dsc = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in va_loader:\n",
        "            x = x.to(device); y = y.to(device)\n",
        "            logits = unet(x)\n",
        "            dscs = dice_per_class(logits, y, args.classes, args.ignore_index)\n",
        "            all_dsc.append(np.array(dscs))\n",
        "    dsc_matrix = np.vstack(all_dsc)\n",
        "    dsc_mean_per_class = np.nanmean(dsc_matrix, axis=0).tolist()\n",
        "    dsc_mean = float(np.nanmean(dsc_matrix))\n",
        "    os.makedirs(args.out, exist_ok=True)\n",
        "    with open(Path(args.out)/\"metrics.json\",\"w\") as f:\n",
        "        json.dump({\"mean_dsc\": dsc_mean, \"dsc_per_class\": dsc_mean_per_class}, f, indent=2)\n",
        "    print(f\"[RESULT] mean DSC={dsc_mean:.4f} | per-class={np.round(dsc_mean_per_class,4)}\")\n",
        "    print(f\"[SAVE] {Path(args.out)/'metrics.json'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}